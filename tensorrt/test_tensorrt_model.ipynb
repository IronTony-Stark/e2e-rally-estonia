{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab82fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/romet/projects/ut/wp4/nvidia-e2e\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0ec628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torchvision\n",
    "#from torchvision import transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from dataloading.nvidia import NvidiaCropWide, Normalize, NvidiaDataset\n",
    "from trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc745f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 300\n",
    "xmax = 1620\n",
    "\n",
    "ymin = 520\n",
    "ymax = 864\n",
    "\n",
    "height = ymax - ymin\n",
    "width = xmax - xmin\n",
    "\n",
    "scale = 0.2\n",
    "scaled_width = int(scale*width)\n",
    "scaled_height = int(scale*height)\n",
    "\n",
    "def crop(img):\n",
    "    return img[ymin:ymax, xmin:xmax, :]\n",
    "\n",
    "def resize(img):\n",
    "    return cv2.resize(img, dsize=(scaled_width, scaled_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def normalise(img):\n",
    "    return (img / 255)\n",
    "\n",
    "def preprocess(img):\n",
    "    img = crop(img)\n",
    "    img = resize(img)\n",
    "    img = normalise(img)\n",
    "    return img\n",
    "\n",
    "def read_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = preprocess(img)\n",
    "    return img\n",
    "\n",
    "def create_tensor(img):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "    img_tensor = torch.FloatTensor(img).to(device).permute(2, 0, 1).unsqueeze(0)\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa74762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = validset.image_paths[0]\n",
    "# img = read_image(image_path)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d5364ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "import os\n",
    "\n",
    "# Simple helper data class that's a little nicer to use than a 2-tuple.\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "class TensorrtModel:\n",
    "    def __init__(self, onnx_file_path):\n",
    "        self.init_tensorrt(onnx_file_path)\n",
    "        \n",
    "    def predict(self, img):\n",
    "        self.inputs[0].host = img\n",
    "        out = self.do_inference_v2(self.context, self.bindings, self.inputs, self.outputs, self.stream)[0]\n",
    "        if type(out) == np.ndarray:\n",
    "            out = out[0]\n",
    "        return out\n",
    "    \n",
    "    # Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\n",
    "    def allocate_buffers(self, engine):\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        bindings = []\n",
    "        stream = cuda.Stream()\n",
    "        for binding in engine:\n",
    "            size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
    "            dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "            # Allocate host and device buffers\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            # Append the device buffer to device bindings.\n",
    "            bindings.append(int(device_mem))\n",
    "            # Append to the appropriate list.\n",
    "            if engine.binding_is_input(binding):\n",
    "                inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "            else:\n",
    "                outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        return inputs, outputs, bindings, stream\n",
    "\n",
    "    # This function is generalized for multiple inputs/outputs for full dimension networks.\n",
    "    # inputs and outputs are expected to be lists of HostDeviceMem objects.\n",
    "    def do_inference_v2(self, context, bindings, inputs, outputs, stream):\n",
    "        # Transfer input data to the GPU.\n",
    "        [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "        # Run inference.\n",
    "        context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "        # Transfer predictions back from the GPU.\n",
    "        [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "        # Synchronize the stream\n",
    "        stream.synchronize()\n",
    "        # Return only the host outputs.\n",
    "        return [out.host for out in outputs]\n",
    "\n",
    "    def build_engine_from_onnx(self, onnx_file_path):\n",
    "        \"\"\"Takes an ONNX file and creates a TensorRT engine to run inference with\"\"\"\n",
    "        EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "        with trt.Logger() as trt_logger, trt.Builder(trt_logger) as builder, builder.create_network(\n",
    "                EXPLICIT_BATCH) as network, trt.OnnxParser(network, trt_logger) as parser:\n",
    "            builder.max_workspace_size = 1 << 28  # 256MiB\n",
    "            builder.max_batch_size = 1\n",
    "            # Parse model file\n",
    "            if not os.path.exists(onnx_file_path):\n",
    "                print('ONNX file {} not found.'.format(onnx_file_path))\n",
    "                exit(0)\n",
    "            print('Loading ONNX file from path {}...'.format(onnx_file_path))\n",
    "            with open(onnx_file_path, 'rb') as model:\n",
    "                print('Beginning ONNX file parsing')\n",
    "                if not parser.parse(model.read()):\n",
    "                    print('ERROR: Failed to parse the ONNX file.')\n",
    "                    for error in range(parser.num_errors):\n",
    "                        print(parser.get_error(error))\n",
    "                    return None\n",
    "            # The actual yolov3.onnx is generated with batch size 64. Reshape input to batch size 1\n",
    "            # network.get_input(0).shape = [1, 3, 128, 1024 ]\n",
    "            print('Completed parsing of ONNX file')\n",
    "            print('Building an engine from file {}; this may take a while...'.format(onnx_file_path))\n",
    "            engine = builder.build_cuda_engine(network)\n",
    "            print(\"Completed creating Engine\")\n",
    "            return engine\n",
    "\n",
    "    def init_tensorrt(self, onnx_file_path):\n",
    "        engine = self.build_engine_from_onnx(onnx_file_path)\n",
    "        self.inputs, self.outputs, self.bindings, self.stream = self.allocate_buffers(engine)\n",
    "        self.context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c1b81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ONNX file from path models/20211124220319_nvidia-v3/best.onnx...\n",
      "Beginning ONNX file parsing\n",
      "Completed parsing of ONNX file\n",
      "Building an engine from file models/20211124220319_nvidia-v3/best.onnx; this may take a while...\n",
      "Completed creating Engine\n"
     ]
    }
   ],
   "source": [
    "model_path = \"models/20211124220319_nvidia-v3\"\n",
    "tensorrt_model = TensorrtModel(f\"{model_path}/best.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f170d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/romet/data2/datasets/rally-estonia/dataset/2021-10-26-10-49-06_e2e_rec_ss20_elva: 33045\n",
      "/home/romet/data2/datasets/rally-estonia/dataset/2021-10-26-10-49-06_e2e_rec_ss20_elva: 33045\n"
     ]
    }
   ],
   "source": [
    "root_path = Path(\"/home/romet/data2/datasets/rally-estonia/dataset\")\n",
    "valid_paths = [root_path / \"2021-10-26-10-49-06_e2e_rec_ss20_elva\"]\n",
    "validset = NvidiaDataset(valid_paths, camera=\"front_wide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eddaf3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ad5588be3b465c8a72bf4f0ca1fd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33045 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_242061/1856928682.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_242061/2577924298.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tensorrt_predictions = []\n",
    "\n",
    "for path in tqdm(validset.frames.image_path.to_numpy()):\n",
    "    img = read_image(path)\n",
    "    img = np.transpose(img, [2, 0, 1])\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = np.array(img, dtype=np.float32, order='C')\n",
    "    prediction = tensorrt_model.predict(img)\n",
    "    tensorrt_predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d93de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\"\")\n",
    "torch_model = trainer.load_model(f\"{model_path}/best.pt\")\n",
    "torch_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f18a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_predictions = []\n",
    "\n",
    "for path in tqdm(validset.frames.image_path.to_numpy()):\n",
    "    img = read_image(path)\n",
    "    img_tensor = create_tensor(img)\n",
    "    prediction = torch_model(img_tensor).squeeze(1)\n",
    "    torch_predictions.append(prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = transforms.Compose([NvidiaCropWide(), Normalize()])\n",
    "validset_tr = NvidiaDataset(valid_paths, tr)\n",
    "torch_transforms_predictions = trainer.predict(torch_model, validset_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 30))\n",
    "\n",
    "ax1.plot(validset.vehicle_speed)\n",
    "ax1.plot(torch_transforms_predictions)\n",
    "ax1.legend([\"ground truth\", \"predictions\"])\n",
    "ax1.set_title(\"PyTorch model with transforms\")\n",
    "\n",
    "ax2.plot(validset.vehicle_speed)\n",
    "ax2.plot(torch_predictions)\n",
    "ax2.legend([\"ground truth\", \"predictions\"])\n",
    "ax2.set_title(\"PyTorch model with numpy/cv2 preprocessing\")\n",
    "\n",
    "ax3.plot(validset.vehicle_speed)\n",
    "ax3.plot(tensorrt_predictions)\n",
    "ax3.legend([\"ground truth\", \"predictions\"])\n",
    "ax3.set_title(\"TensorRT\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2\n",
    "import shutil\n",
    "from skimage import io\n",
    "import os\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "\n",
    "def draw_steering_angle(frame, steering_angle, steering_wheel_radius, steering_position, size, color):\n",
    "    steering_angle_rad = math.radians(steering_angle)\n",
    "    x = steering_wheel_radius * np.cos(np.pi / 2 + steering_angle_rad)\n",
    "    y = steering_wheel_radius * np.sin(np.pi / 2 + steering_angle_rad)\n",
    "    cv2.circle(frame, (steering_position[0] + int(x), steering_position[1] - int(y)), size, color, thickness=-1)\n",
    "    \n",
    "def draw_frames(dataset, predicted_angles, temp_frames_folder):\n",
    "    \n",
    "    for frame_index, data in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "        frame = data[\"image\"].permute(1, 2, 0).cpu().numpy()\n",
    "        true_angle = math.degrees(data[\"steering_angle\"])\n",
    "        pred_angle = math.degrees(predicted_angles[frame_index])\n",
    "        \n",
    "        cv2.putText(frame, 'True: {:.2f}deg'.format(true_angle), (10, 1150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2,\n",
    "                    cv2.LINE_AA)\n",
    "        cv2.putText(frame, 'Pred: {:.2f}deg'.format(pred_angle), (10, 1200), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2,\n",
    "                    cv2.LINE_AA)\n",
    "    \n",
    "        radius = 200\n",
    "        steering_pos = (960, 1200)\n",
    "        cv2.circle(frame, steering_pos, radius, (255, 255, 255), 7)\n",
    "\n",
    "        draw_steering_angle(frame, true_angle, radius, steering_pos, 13, (0, 255, 0))\n",
    "        draw_steering_angle(frame, pred_angle, radius, steering_pos, 9, (255, 0, 0))\n",
    "            \n",
    "        io.imsave(f\"{temp_frames_folder}/{frame_index + 1:05}.jpg\", frame)\n",
    "        \n",
    "\n",
    "def convert_frames_to_video(frames_folder, output_video_path, fps=25):\n",
    "    output_folder = Path(os.path.split(output_video_path)[:-1][0])\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    p = Path(frames_folder).glob('**/*.jpg')\n",
    "    image_list = sorted([str(x) for x in p if x.is_file()])\n",
    "\n",
    "    print(\"Creating video {}, FPS={}\".format(frames_folder, fps))\n",
    "    clip = ImageSequenceClip(image_list, fps=fps)\n",
    "    clip.write_videofile(output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ce918",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_frames_folder = Path(\"./temp_frames\")\n",
    "shutil.rmtree(temp_frames_folder, ignore_errors=True)\n",
    "temp_frames_folder.mkdir()\n",
    "\n",
    "draw_frames(validset, tensorrt_predictions, temp_frames_folder)\n",
    "output_video = \"output/wide-v2-ss6.mp4\"\n",
    "convert_frames_to_video(temp_frames_folder, output_video, fps=30.0)\n",
    "\n",
    "shutil.rmtree(temp_frames_folder, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994d77f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{output_video}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d68492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
