{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d0ec628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from dataloading.nvidia import NvidiaResizeAndCrop, Normalize, NvidiaDataset\n",
    "from trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c88ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/1-pilotnet-base/1cam-batch-v5\"\n",
    "\n",
    "root_path = Path(\"/home/romet/data/datasets/ut/nvidia-data\")\n",
    "valid_paths = [root_path / \"2021-05-28-15-19-48_e2e_sulaoja_20_30\"]\n",
    "validset = NvidiaDataset(valid_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc745f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(img):\n",
    "    width = 258\n",
    "    height = 66\n",
    "    return cv2.resize(img, dsize=(width, height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def normalise(img):\n",
    "    z = (img / 255)\n",
    "    #z[..., :] -= [0.485, 0.456, 0.406]\n",
    "    #z[..., :] /= [0.229, 0.224, 0.225]\n",
    "\n",
    "    return z\n",
    "\n",
    "def crop(img):\n",
    "    return img[600:969, 186:1734, :]\n",
    "\n",
    "def preprocess(img):\n",
    "    img = crop(img)\n",
    "    img = resize(img)\n",
    "    img = normalise(img)\n",
    "    return img\n",
    "\n",
    "def read_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = preprocess(img)\n",
    "    return img\n",
    "\n",
    "def create_tensor(img):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "    img_tensor = torch.FloatTensor(img).to(device).permute(2, 0, 1).unsqueeze(0)\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5364ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "import os\n",
    "\n",
    "# Simple helper data class that's a little nicer to use than a 2-tuple.\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "class TensorrtModel:\n",
    "    def __init__(self, onnx_file_path):\n",
    "        self.init_tensorrt(onnx_file_path)\n",
    "        \n",
    "    def predict(self, img):\n",
    "        self.inputs[0].host = img\n",
    "        out = self.do_inference_v2(self.context, self.bindings, self.inputs, self.outputs, self.stream)[0]\n",
    "        if type(out) == np.ndarray:\n",
    "            out = out[0]\n",
    "        return out\n",
    "    \n",
    "    # Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\n",
    "    def allocate_buffers(self, engine):\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        bindings = []\n",
    "        stream = cuda.Stream()\n",
    "        for binding in engine:\n",
    "            size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
    "            dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "            # Allocate host and device buffers\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            # Append the device buffer to device bindings.\n",
    "            bindings.append(int(device_mem))\n",
    "            # Append to the appropriate list.\n",
    "            if engine.binding_is_input(binding):\n",
    "                inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "            else:\n",
    "                outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        return inputs, outputs, bindings, stream\n",
    "\n",
    "    # This function is generalized for multiple inputs/outputs for full dimension networks.\n",
    "    # inputs and outputs are expected to be lists of HostDeviceMem objects.\n",
    "    def do_inference_v2(self, context, bindings, inputs, outputs, stream):\n",
    "        # Transfer input data to the GPU.\n",
    "        [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "        # Run inference.\n",
    "        context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "        # Transfer predictions back from the GPU.\n",
    "        [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "        # Synchronize the stream\n",
    "        stream.synchronize()\n",
    "        # Return only the host outputs.\n",
    "        return [out.host for out in outputs]\n",
    "\n",
    "    def build_engine_from_onnx(self, onnx_file_path):\n",
    "        \"\"\"Takes an ONNX file and creates a TensorRT engine to run inference with\"\"\"\n",
    "        EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "        with trt.Logger() as trt_logger, trt.Builder(trt_logger) as builder, builder.create_network(\n",
    "                EXPLICIT_BATCH) as network, trt.OnnxParser(network, trt_logger) as parser:\n",
    "            builder.max_workspace_size = 1 << 28  # 256MiB\n",
    "            builder.max_batch_size = 1\n",
    "            # Parse model file\n",
    "            if not os.path.exists(onnx_file_path):\n",
    "                print('ONNX file {} not found.'.format(onnx_file_path))\n",
    "                exit(0)\n",
    "            print('Loading ONNX file from path {}...'.format(onnx_file_path))\n",
    "            with open(onnx_file_path, 'rb') as model:\n",
    "                print('Beginning ONNX file parsing')\n",
    "                if not parser.parse(model.read()):\n",
    "                    print('ERROR: Failed to parse the ONNX file.')\n",
    "                    for error in range(parser.num_errors):\n",
    "                        print(parser.get_error(error))\n",
    "                    return None\n",
    "            # The actual yolov3.onnx is generated with batch size 64. Reshape input to batch size 1\n",
    "            # network.get_input(0).shape = [1, 3, 128, 1024 ]\n",
    "            print('Completed parsing of ONNX file')\n",
    "            print('Building an engine from file {}; this may take a while...'.format(onnx_file_path))\n",
    "            engine = builder.build_cuda_engine(network)\n",
    "            print(\"Completed creating Engine\")\n",
    "            return engine\n",
    "\n",
    "    def init_tensorrt(self, onnx_file_path):\n",
    "        engine = self.build_engine_from_onnx(onnx_file_path)\n",
    "        self.inputs, self.outputs, self.bindings, self.stream = self.allocate_buffers(engine)\n",
    "        self.context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c1b81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ONNX file from path models/OusterNet_May06_17-06-23.onnx...\n",
      "Beginning ONNX file parsing\n",
      "Completed parsing of ONNX file\n",
      "Building an engine from file models/OusterNet_May06_17-06-23.onnx; this may take a while...\n",
      "Completed creating Engine\n"
     ]
    }
   ],
   "source": [
    "#tensorrt_model = TensorrtModel(f\"{model_path}/best.onnx\")\n",
    "tensorrt_model = TensorrtModel(\"models/OusterNet_May06_17-06-23.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddaf3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b88f26f303746bf9906de1d08a669e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10709 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorrt_predictions = []\n",
    "\n",
    "for path in tqdm(validset.image_paths):\n",
    "    img = read_image(path)\n",
    "    img = np.transpose(img, [2, 0, 1])\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = np.array(img, dtype=np.float32, order='C')\n",
    "    prediction = tensorrt_model.predict(img)\n",
    "    tensorrt_predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8d93de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PilotNet(\n",
       "  (conv1): Conv2d(3, 24, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (cbn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(24, 36, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (cbn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(36, 48, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (cbn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (cbn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (cbn5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lin1): Linear(in_features=1600, out_features=100, bias=True)\n",
       "  (lin2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (lin3): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (lin4): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (lr): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\"\")\n",
    "torch_model = trainer.load_model(f\"{model_path}/best.pt\")\n",
    "torch_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f18a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82580040c2714853a93f2d898518d99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10709 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch_predictions = []\n",
    "\n",
    "for path in tqdm(validset.image_paths):\n",
    "    img = read_image(path)\n",
    "    img_tensor = create_tensor(img)\n",
    "    prediction = torch_model(img_tensor).squeeze(1)\n",
    "    torch_predictions.append(prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = transforms.Compose([NvidiaResizeAndCrop(), Normalize()])\n",
    "validset_tr = NvidiaDataset(valid_paths, tr)\n",
    "torch_transforms_predictions = trainer.predict(torch_model, validset_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(21, 7))\n",
    "\n",
    "ax1.plot(validset.steering_angles)\n",
    "#ax1.plot(torch_transforms_predictions)\n",
    "ax1.legend([\"ground truth\", \"predictions\"])\n",
    "ax1.set_title(\"PyTorch model with transforms\")\n",
    "\n",
    "ax2.plot(validset.steering_angles)\n",
    "ax2.plot(torch_predictions)\n",
    "ax2.legend([\"ground truth\", \"predictions\"])\n",
    "ax2.set_title(\"PyTorch model with numpy/cv2 preprocessing\")\n",
    "\n",
    "ax3.plot(validset.steering_angles)\n",
    "#ax3.plot(tensorrt_predictions)\n",
    "ax3.legend([\"ground truth\", \"predictions\"])\n",
    "ax3.set_title(\"TensorRT\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2\n",
    "import shutil\n",
    "from skimage import io\n",
    "import os\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "\n",
    "def draw_steering_angle(frame, steering_angle, steering_wheel_radius, steering_position, size, color):\n",
    "    steering_angle_rad = math.radians(steering_angle)\n",
    "    x = steering_wheel_radius * np.cos(np.pi / 2 + steering_angle_rad)\n",
    "    y = steering_wheel_radius * np.sin(np.pi / 2 + steering_angle_rad)\n",
    "    cv2.circle(frame, (steering_position[0] + int(x), steering_position[1] - int(y)), size, color, thickness=-1)\n",
    "    \n",
    "def draw_frames(dataset, predicted_angles, temp_frames_folder):\n",
    "    \n",
    "    for frame_index, data in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "        frame = data[\"image\"].permute(1, 2, 0).cpu().numpy()\n",
    "        true_angle = math.degrees(data[\"steering_angle\"])\n",
    "        pred_angle = math.degrees(predicted_angles[frame_index])\n",
    "        \n",
    "        cv2.putText(frame, 'True: {:.2f}deg'.format(true_angle), (10, 1150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2,\n",
    "                    cv2.LINE_AA)\n",
    "        cv2.putText(frame, 'Pred: {:.2f}deg'.format(pred_angle), (10, 1200), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2,\n",
    "                    cv2.LINE_AA)\n",
    "    \n",
    "        radius = 200\n",
    "        steering_pos = (960, 1200)\n",
    "        cv2.circle(frame, steering_pos, radius, (255, 255, 255), 7)\n",
    "\n",
    "        draw_steering_angle(frame, true_angle, radius, steering_pos, 13, (0, 255, 0))\n",
    "        draw_steering_angle(frame, pred_angle, radius, steering_pos, 9, (255, 0, 0))\n",
    "            \n",
    "        io.imsave(f\"{temp_frames_folder}/{frame_index + 1:05}.jpg\", frame)\n",
    "        \n",
    "\n",
    "def convert_frames_to_video(frames_folder, output_video_path, fps=25):\n",
    "    output_folder = Path(os.path.split(output_video_path)[:-1][0])\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    p = Path(frames_folder).glob('**/*.jpg')\n",
    "    image_list = sorted([str(x) for x in p if x.is_file()])\n",
    "\n",
    "    print(\"Creating video {}, FPS={}\".format(frames_folder, fps))\n",
    "    clip = ImageSequenceClip(image_list, fps=fps)\n",
    "    clip.write_videofile(output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ce918",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_frames_folder = Path(\"./temp_frames\")\n",
    "shutil.rmtree(temp_frames_folder, ignore_errors=True)\n",
    "temp_frames_folder.mkdir()\n",
    "\n",
    "draw_frames(validset, torch_transforms_predictions, temp_frames_folder)\n",
    "output_video = \"output/test3.mp4\"\n",
    "convert_frames_to_video(temp_frames_folder, output_video, fps=30)\n",
    "\n",
    "shutil.rmtree(temp_frames_folder, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994d77f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{output_video}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c6515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
